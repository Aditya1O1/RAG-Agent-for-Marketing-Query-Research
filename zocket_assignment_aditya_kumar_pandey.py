# -*- coding: utf-8 -*-
"""Zocket Assignment Aditya Kumar Pandey.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lM2VicTxQoMeYsfKO_HhcaxKHocP_HRz

Phase 1: Environment Setup on Colab
"""

# Installing necessary libraries
!pip install fastapi uvicorn nest-asyncio pyngrok
!pip install chromadb sentence-transformers
!pip install transformers accelerate
!pip install torch --upgrade


!pip install httpx

from transformers import AutoTokenizer, AutoModelForCausalLM
from huggingface_hub import login

!pip uninstall -y transformers

!pip install --upgrade transformers

!pip install -U transformers accelerate

!pip install torchvision

model_name = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"

!pip install -U transformers accelerate

from transformers import AutoTokenizer, AutoModelForCausalLM

model_name = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"

print("Loading model and tokenizer...")
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name, device_map="auto")
prompt = prompt = "Write a playful, catchy ad copy for a summer sneaker sale with discounts, limited-time offers, and free shipping. Ad Copy:"

inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

print("Generating output...")
outputs = model.generate(
    **inputs,
    max_new_tokens=150,
    do_sample=True,
    temperature=0.7,
    eos_token_id=tokenizer.eos_token_id,
    pad_token_id=tokenizer.pad_token_id
)

print(tokenizer.decode(outputs[0], skip_special_tokens=True))



"""Phase2: ChromaDB + SentenceTransformers Setup"""

!pip install chromadb sentence-transformers



from sentence_transformers import SentenceTransformer
import chromadb

#Here We need to initialize Persistent ChromaDB Client
client = chromadb.PersistentClient(path="./chroma_db")

# Create the collection
collection = client.get_or_create_collection("marketing_blogs")

# Load embedder
embedder = SentenceTransformer("all-MiniLM-L6-v2")

# Sample marketing blog chunks
blog_chunks = [
    "Best practices for writing high-converting Facebook ad copy during seasonal sales.",
    "Tips to reduce cost-per-click in Google Ads campaigns without reducing reach.",
    "How to increase click-through rates on Instagram story ads using dynamic visuals.",
    "Effective call-to-action phrases that drive conversions in e-commerce ads.",
    "Importance of A/B testing for ad creatives on social media platforms.",
    "Retargeting strategies to improve ROI for Facebook and Instagram campaigns.",
    "Optimizing ad landing pages for better user experience and higher conversions.",
    "Understanding your target audience for crafting relevant ad messaging.",
    "Tips for creating engaging video ads for YouTube and Instagram reels.",
    "Analyzing ad performance metrics to improve future campaign strategies."
]
# Generate embeddings
embeddings = embedder.encode(blog_chunks).tolist()

try:
    collection.delete(where={})
except:
    pass

# Insert data into ChromaDB
for idx, (text, emb) in enumerate(zip(blog_chunks, embeddings)):
    collection.add(
        ids=[f"blog_{idx}"],
        documents=[text],
        embeddings=[emb],
        metadatas=[{"source": "sample_blog", "index": idx}]
    )

print(f" Successfully added {len(blog_chunks)} blog chunks to ChromaDB (stored in './chroma_db').")

# Verification: we need to check what is stored
results = collection.get()
print(f"\nStored IDs: {results['ids']}")
print(f"Stored Documents (preview): {[doc[:60] for doc in results['documents']]}")
print(f"Stored Metadata: {results['metadatas']}")



"""Phase 3: RAG Pipeline

"""

# Imports
from sentence_transformers import SentenceTransformer
import chromadb
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

from sentence_transformers import SentenceTransformer
import chromadb
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

# Here I am Initializing persistent ChromaDB client
client = chromadb.PersistentClient(path="./chroma_db")
collection = client.get_collection("marketing_blogs")

# Loading embedder
embedder = SentenceTransformer("all-MiniLM-L6-v2")

# Loading TinyLLaMA model and tokenizer
model_name = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    torch_dtype=torch.float16
)

# User query
query = "How can I improve click-through rates on Instagram ads during summer sales campaigns?"

# Fresh embedding
query_embedding = embedder.encode([query]).tolist()[0]

# Now it will retrieve top-3 relevant chunks
results = collection.query(
    query_embeddings=[query_embedding],
    n_results=3
)

retrieved_chunks = results['documents'][0]
context = "\n".join(retrieved_chunks)

# Fresh prompt
prompt = f"""
You are a marketing assistant. Using the following context, answer the user's question concisely and helpfully, focusing only on the answer without repeating the context or the question.

Context:
{context}

Question: {query}

Answer:
"""

# Now this code will tokenize and generate
inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

with torch.no_grad():
    output = model.generate(
        **inputs,
        max_new_tokens=250,
        temperature=0.7,
        do_sample=True,
        eos_token_id=tokenizer.eos_token_id,
        pad_token_id=tokenizer.eos_token_id
    )

generated_text = tokenizer.decode(output[0], skip_special_tokens=True)

# Displaying output
print("\nðŸ“„ Generated Answer:\n")
print(generated_text)



"""Phase 4: FastAPI Endpoint"""

!pip install fastapi uvicorn nest_asyncio pyngrok

!ngrok config add-authtoken 2yMN4GMKS6s6c4qH5iSCm2ieiIR_3KGhaEMmUbndyr7ZjVPe4

import nest_asyncio
nest_asyncio.apply()

from fastapi import FastAPI
from fastapi.responses import JSONResponse
from pydantic import BaseModel

import chromadb
from sentence_transformers import SentenceTransformer

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

from pyngrok import ngrok
import uvicorn

# Here we need to initialize FastAPI app
app = FastAPI(title="Zocket RAG Agent")

# Initializion of ChromaDB client
client = chromadb.PersistentClient(path="./chroma_db")
collection = client.get_collection("marketing_blogs")

# Loading embedder
embedder = SentenceTransformer("all-MiniLM-L6-v2")

# Loading TinyLLaMA
model_name = "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    torch_dtype=torch.float16
)

#Use Pydantic for structured input
class QueryRequest(BaseModel):
    query: str

@app.post("/run-agent")
async def run_agent(req: QueryRequest):
    try:
        query = req.query.strip()

        if not query:
            return JSONResponse(content={"error": "Query is required."}, status_code=400)

        # query emebedding
        query_embedding = embedder.encode([query]).tolist()[0]

        # Retrieve top-3 relevant chunks
        results = collection.query(
            query_embeddings=[query_embedding],
            n_results=3
        )

        retrieved_chunks = results['documents'][0]
        context = "\n".join(retrieved_chunks)

        # Creating prompt
        prompt = f"""
You are a marketing assistant. Using the following context, answer the user's question concisely and helpfully, focusing only on the answer without repeating the context or the question.

Context:
{context}

Question: {query}

Answer:
"""

        # Tokenize and generate
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

        with torch.no_grad():
            output = model.generate(
                **inputs,
                max_new_tokens=250,
                temperature=0.7,
                do_sample=True,
                eos_token_id=tokenizer.eos_token_id,
                pad_token_id=tokenizer.eos_token_id
            )

        generated_text = tokenizer.decode(output[0], skip_special_tokens=True)

        return JSONResponse(content={
            "query": query,
            "response": generated_text.strip()
        })

    except Exception as e:
        return JSONResponse(content={"error": str(e)}, status_code=500)

# ngrok tunnel
public_url = ngrok.connect(8000)
print(f" FastAPI server is running! Access your Zocket RAG Agent here:\n{public_url}/docs")

# Run the FastAPI server
uvicorn.run(app, port=8000)

#https://73af-34-142-204-37.ngrok-free.app/docs



















